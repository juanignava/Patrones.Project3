{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 3 - Aprendizaje no supervisado\n",
    "\n",
    "- Juan Ignacio Navarro\n",
    "- Jose David SÃ¡nchez\n",
    "- Steven Badilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Used libraries for the entire project\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random \n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import optuna\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, MaxPooling2D, UpSampling2D, Reshape\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set de datos llamado Plant_leaf_deseases_dataset_without_augmentation: https://data.mendeley.com/datasets/tywbtsjrjv/1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop and fix Bias in images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Delete unnecessary images --\n",
    "\n",
    "These background images do not help the model on anything\n",
    "\"\"\"\n",
    "\n",
    "os.remove(\"images/original_dataset/Background_without_leaves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Delete images from cropped dataset --\n",
    "\n",
    "This cell deletes the files in the cropped dataset if any. The\n",
    "cropped dataset a cropped copy from the additional dataset images.\n",
    "The images are cropped to show only the most important part of \n",
    "the leaves which is the center\n",
    "\"\"\"\n",
    "\n",
    "cropped_images_folder = 'images/cropped_dataset/'\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    folder_path = os.path.join(cropped_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# remove the files in each category\n",
    "for category in categories.keys():\n",
    "\n",
    "    folder_path = os.path.join(cropped_images_folder, category)\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(cropped_images_folder, category, file_name)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Crop images borders -- \n",
    "\n",
    "This block crops the images borders and saves the new images into the cropped_dataset\n",
    "\"\"\"\n",
    "\n",
    "# define a different folder to save the cropped images\n",
    "original_images_folder = 'images/original_dataset/'\n",
    "analysis_images_folder = 'images/cropped_dataset/'\n",
    "\n",
    "# define the new size\n",
    "target_size = (100, 100)\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(original_images_folder))):\n",
    "    folder_path = os.path.join(original_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# Create category folders in cropped dataset\n",
    "for index, folder_name in enumerate(sorted(os.listdir(original_images_folder))):\n",
    "    folder_path = os.path.join(cropped_images_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# crop and save the cropped images\n",
    "for category in categories.keys():\n",
    "\n",
    "    cat_files = os.listdir(os.path.join(original_images_folder, category))\n",
    "\n",
    "    for i, file in enumerate(cat_files):\n",
    "\n",
    "        # constructing image path\n",
    "        input_path = os.path.join(original_images_folder, category, file)\n",
    "        image = Image.open(input_path)\n",
    "\n",
    "        if image.size[0] < 256 or image.size[1] < 256:\n",
    "            continue\n",
    "        \n",
    "        # get original image size and calculate borders\n",
    "        width, height = image.size\n",
    "        left = (width - target_size[0]) // 2\n",
    "        upper = (height - target_size[1]) // 2\n",
    "        right = left + target_size[0]\n",
    "        lower = upper + target_size[1]\n",
    "\n",
    "        # crop the image\n",
    "        cropped_image = image.crop((left, upper, right, lower))\n",
    "        output_path = os.path.join(cropped_images_folder, category, file)\n",
    "        cropped_image.save(output_path)\n",
    "        image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of images in analysis dataset:\n",
      "Apple___Apple_scab                                :   630\n",
      "Apple___Black_rot                                 :   621\n",
      "Apple___Cedar_apple_rust                          :   275\n",
      "Apple___healthy                                   :  1645\n",
      "Blueberry___healthy                               :  1502\n",
      "Cherry___Powdery_mildew                           :  1052\n",
      "Cherry___healthy                                  :   854\n",
      "Corn___Cercospora_leaf_spot Gray_leaf_spot        :   513\n",
      "Corn___Common_rust                                :  1192\n",
      "Corn___Northern_Leaf_Blight                       :   985\n",
      "Corn___healthy                                    :  1162\n",
      "Grape___Black_rot                                 :  1180\n",
      "Grape___Esca_(Black_Measles)                      :  1383\n",
      "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)        :  1076\n",
      "Grape___healthy                                   :   423\n",
      "Orange___Haunglongbing_(Citrus_greening)          :  5507\n",
      "Peach___Bacterial_spot                            :  2297\n",
      "Peach___healthy                                   :   360\n",
      "Pepper,_bell___Bacterial_spot                     :   997\n",
      "Pepper,_bell___healthy                            :  1477\n",
      "Potato___Early_blight                             :  1000\n",
      "Potato___Late_blight                              :  1000\n",
      "Potato___healthy                                  :   152\n",
      "Raspberry___healthy                               :   371\n",
      "Soybean___healthy                                 :  5090\n",
      "Squash___Powdery_mildew                           :  1835\n",
      "Strawberry___Leaf_scorch                          :  1109\n",
      "Strawberry___healthy                              :   456\n",
      "Tomato___Bacterial_spot                           :  2127\n",
      "Tomato___Early_blight                             :  1000\n",
      "Tomato___Late_blight                              :  1909\n",
      "Tomato___Leaf_Mold                                :   952\n",
      "Tomato___Septoria_leaf_spot                       :  1771\n",
      "Tomato___Spider_mites Two-spotted_spider_mite     :  1676\n",
      "Tomato___Target_Spot                              :  1404\n",
      "Tomato___Tomato_Yellow_Leaf_Curl_Virus            :  5357\n",
      "Tomato___Tomato_mosaic_virus                      :   373\n",
      "Tomato___healthy                                  :  1591\n",
      "Category with the minimum amount of images: Potato___healthy with 152\n",
      "Category with the maximim amount of images: Orange___Haunglongbing_(Citrus_greening) with 5507\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Check Bias --\n",
    "\n",
    "Calculate the amount of images to check for possible Bias in the dataset\n",
    "\"\"\"\n",
    "\n",
    "cropped_images_folder = 'images/cropped_dataset/'\n",
    "category_amount = []\n",
    "min_bias = 10e4\n",
    "min_bias_key = ''\n",
    "max_bias = 0\n",
    "max_bias_key = ''\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    folder_path = os.path.join(cropped_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# get the amount of images for each category\n",
    "for category in categories.keys():\n",
    "    folder_path = os.path.join(cropped_images_folder, category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    folder_count = len(image_files)\n",
    "    if folder_count > max_bias:\n",
    "        max_bias = folder_count\n",
    "        max_bias_key = category\n",
    "    if folder_count < min_bias:\n",
    "        min_bias = folder_count\n",
    "        min_bias_key = category\n",
    "    category_amount.append(folder_count)\n",
    "\n",
    "# Print the information taken\n",
    "print(\"Amount of images in analysis dataset:\")\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    print(f\"{folder_name:50}: {category_amount[index]:5d}\")\n",
    "\n",
    "print(f\"Category with the minimum amount of images: {min_bias_key} with {min_bias}\")\n",
    "print(f\"Category with the maximim amount of images: {max_bias_key} with {max_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Decrease bias by combining images --\n",
    "\n",
    "By combining pixels from the images categories it will be possible to add more images to the categories\n",
    "that need more data\n",
    "\"\"\"\n",
    "\n",
    "cropped_dataset_folder = \"images/cropped_dataset/\"\n",
    "\n",
    "max_bias_folder = f\"{cropped_dataset_folder}/{max_bias_key}/\"\n",
    "\n",
    "AMOUNT_IMAGES = 500   \n",
    "\n",
    "folders = os.listdir(cropped_dataset_folder)\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(cropped_dataset_folder, folder)\n",
    "    amount_images = len(os.listdir(folder_path))\n",
    "\n",
    "    if amount_images > 1.5 * min_bias: continue\n",
    "\n",
    "    random_images = random.sample(os.listdir(folder_path), min_bias)\n",
    "\n",
    "    counter = 0\n",
    "    counter_random = 0\n",
    "    while(counter < AMOUNT_IMAGES):\n",
    "\n",
    "        for image in os.listdir(folder_path):\n",
    "\n",
    "            if counter % min_bias == 0: counter_random += 1\n",
    "\n",
    "            if (counter == AMOUNT_IMAGES): break\n",
    "            random_image_path = os.path.join(max_bias_folder, random_images[counter_random])\n",
    "            other_folder_image_path = os.path.join(folder_path, image)\n",
    "\n",
    "            image1 = Image.open(random_image_path)\n",
    "            image2 = Image.open(other_folder_image_path)\n",
    "\n",
    "            if image1.size == (100, 100, 3) or image2.size == (100, 100, 3): continue\n",
    "\n",
    "            image1.resize((100, 100))\n",
    "            image2.resize((100, 100))\n",
    "\n",
    "            array1 = np.array(image1)\n",
    "            array2 = np.array(image2)\n",
    "\n",
    "            average_array = (0.5 * array1 + 0.5 * array2).astype(np.uint8)\n",
    "            combined_image = Image.fromarray(average_array)\n",
    "            \n",
    "            combined_image_path = os.path.join(folder_path, f\"combined_{counter}.JPG\")\n",
    "            combined_image.save(combined_image_path)\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of images in analysis dataset:\n",
      "Apple___Apple_scab                                :   630\n",
      "Apple___Black_rot                                 :   621\n",
      "Apple___Cedar_apple_rust                          :   275\n",
      "Apple___healthy                                   :  1645\n",
      "Blueberry___healthy                               :  1502\n",
      "Cherry___Powdery_mildew                           :  1052\n",
      "Cherry___healthy                                  :   854\n",
      "Corn___Cercospora_leaf_spot Gray_leaf_spot        :   513\n",
      "Corn___Common_rust                                :  1192\n",
      "Corn___Northern_Leaf_Blight                       :   985\n",
      "Corn___healthy                                    :  1162\n",
      "Grape___Black_rot                                 :  1180\n",
      "Grape___Esca_(Black_Measles)                      :  1383\n",
      "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)        :  1076\n",
      "Grape___healthy                                   :   423\n",
      "Orange___Haunglongbing_(Citrus_greening)          :  5507\n",
      "Peach___Bacterial_spot                            :  2297\n",
      "Peach___healthy                                   :   360\n",
      "Pepper,_bell___Bacterial_spot                     :   997\n",
      "Pepper,_bell___healthy                            :  1477\n",
      "Potato___Early_blight                             :  1000\n",
      "Potato___Late_blight                              :  1000\n",
      "Potato___healthy                                  :   652\n",
      "Raspberry___healthy                               :   371\n",
      "Soybean___healthy                                 :  5090\n",
      "Squash___Powdery_mildew                           :  1835\n",
      "Strawberry___Leaf_scorch                          :  1109\n",
      "Strawberry___healthy                              :   456\n",
      "Tomato___Bacterial_spot                           :  2127\n",
      "Tomato___Early_blight                             :  1000\n",
      "Tomato___Late_blight                              :  1909\n",
      "Tomato___Leaf_Mold                                :   952\n",
      "Tomato___Septoria_leaf_spot                       :  1771\n",
      "Tomato___Spider_mites Two-spotted_spider_mite     :  1676\n",
      "Tomato___Target_Spot                              :  1404\n",
      "Tomato___Tomato_Yellow_Leaf_Curl_Virus            :  5357\n",
      "Tomato___Tomato_mosaic_virus                      :   373\n",
      "Tomato___healthy                                  :  1591\n",
      "Category with the minimum amount of images: Apple___Cedar_apple_rust with 275\n",
      "Category with the maximim amount of images: Orange___Haunglongbing_(Citrus_greening) with 5507\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Check Bias --\n",
    "\n",
    "Calculate the amount of images to check for possible Bias in the dataset\n",
    "\"\"\"\n",
    "cropped_images_folder = 'images/cropped_dataset/'\n",
    "category_amount = []\n",
    "min_bias = 10e4\n",
    "min_bias_key = ''\n",
    "max_bias = 0\n",
    "max_bias_key = ''\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    folder_path = os.path.join(cropped_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# get the amount of images for each category\n",
    "for category in categories.keys():\n",
    "    folder_path = os.path.join(cropped_images_folder, category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    folder_count = len(image_files)\n",
    "    if folder_count > max_bias:\n",
    "        max_bias = folder_count\n",
    "        max_bias_key = category\n",
    "    if folder_count < min_bias:\n",
    "        min_bias = folder_count\n",
    "        min_bias_key = category\n",
    "    category_amount.append(folder_count)\n",
    "\n",
    "# Print the information taken\n",
    "print(\"Amount of images in analysis dataset:\")\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    print(f\"{folder_name:50}: {category_amount[index]:5d}\")\n",
    "\n",
    "print(f\"Category with the minimum amount of images: {min_bias_key} with {min_bias}\")\n",
    "print(f\"Category with the maximim amount of images: {max_bias_key} with {max_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Delete images from analysis dataset --\n",
    "\n",
    "This cell deletes the files in the cropped dataset if any. The\n",
    "cropped dataset a cropped copy from the additional dataset images.\n",
    "The images are cropped to show only the most important part of \n",
    "the leaves which is the center\n",
    "\"\"\"\n",
    "\n",
    "analysis_images_folder = 'images/analysis_dataset/'\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(analysis_images_folder))):\n",
    "    folder_path = os.path.join(analysis_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# remove the files\n",
    "for category in categories.keys():\n",
    "\n",
    "    folder_path = os.path.join(analysis_images_folder, category)\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(analysis_images_folder, category, file_name)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Move images to analysis dataset --\n",
    "\n",
    "This ignores the extra images in the Bias\n",
    "\"\"\"\n",
    "\n",
    "# define a different folder to save the cropped images\n",
    "analysis_images_folder = 'images/analysis_dataset/'\n",
    "cropped_images_folder = 'images/cropped_dataset/'\n",
    "\n",
    "# get categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    folder_path = os.path.join(cropped_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# Create category folders in cropped dataset\n",
    "for index, folder_name in enumerate(sorted(os.listdir(cropped_images_folder))):\n",
    "    folder_path = os.path.join(analysis_images_folder, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for category in categories.keys():\n",
    "\n",
    "    cropped_category_folder = os.path.join(cropped_images_folder, category)\n",
    "    analysis_category_folder = os.path.join(analysis_images_folder, category)\n",
    "\n",
    "    for i, image in enumerate(os.listdir(cropped_category_folder)):\n",
    "\n",
    "        #if i == min_bias*4: break\n",
    "\n",
    "        cropped_image_path = os.path.join(cropped_category_folder, image)\n",
    "        analysis_image_path = os.path.join(analysis_category_folder, image)\n",
    "\n",
    "        shutil.copy(cropped_image_path, analysis_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category with the minimum amount of images: Apple___Cedar_apple_rust with 275\n",
      "Category with the maximim amount of images: Orange___Haunglongbing_(Citrus_greening) with 5507\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Check Bias --\n",
    "\n",
    "Calculate the amount of images to check for possible Bias in the dataset\n",
    "\"\"\"\n",
    "\n",
    "analysis_images_folder = 'images/analysis_dataset/'\n",
    "category_amount = []\n",
    "min_bias = 10e4\n",
    "min_bias_key = ''\n",
    "max_bias = 0\n",
    "max_bias_key = ''\n",
    "\n",
    "categories = {}\n",
    "# Iterate over the contents of the \"original_dataset\" folder\n",
    "for index, folder_name in enumerate(sorted(os.listdir(analysis_images_folder))):\n",
    "    folder_path = os.path.join(analysis_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# get the amount of images for each category\n",
    "for category in categories.keys():\n",
    "    folder_path = os.path.join(analysis_images_folder, category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    folder_count = len(image_files)\n",
    "    if folder_count > max_bias:\n",
    "        max_bias = folder_count\n",
    "        max_bias_key = category\n",
    "    if folder_count < min_bias:\n",
    "        min_bias = folder_count\n",
    "        min_bias_key = category\n",
    "    category_amount.append(folder_count)\n",
    "\n",
    "print(f\"Category with the minimum amount of images: {min_bias_key} with {min_bias}\")\n",
    "print(f\"Category with the maximim amount of images: {max_bias_key} with {max_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Delete contents in resized_dataset folder -- \n",
    "\"\"\"\n",
    "resized_images_folder = 'images/resized_dataset/'\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(resized_images_folder))):\n",
    "    folder_path = os.path.join(resized_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# remove the files\n",
    "for category in categories.keys():\n",
    "\n",
    "    folder_path = os.path.join(resized_images_folder, category)\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(resized_images_folder, category, file_name)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Resize images to 64 x 64 --\n",
    "\"\"\"\n",
    "\n",
    "analysis_images_folder = 'images/analysis_dataset/'\n",
    "resized_images_folder = 'images/resized_dataset/'\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(analysis_images_folder))):\n",
    "    folder_path = os.path.join(analysis_images_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "for category in categories.keys():\n",
    "\n",
    "    analysis_category_folder = os.path.join(analysis_images_folder, category)\n",
    "    resized_category_folder = os.path.join(resized_images_folder, category)\n",
    "\n",
    "    if not os.path.exists(resized_category_folder):\n",
    "        os.mkdir(resized_category_folder)\n",
    "\n",
    "    for image in os.listdir(analysis_category_folder):\n",
    "\n",
    "        analysis_image_path = os.path.join(analysis_category_folder, image)\n",
    "        resized_image_path = os.path.join(resized_category_folder, image)\n",
    "\n",
    "        desired_size = (64, 64)\n",
    "        image = Image.open(analysis_image_path)\n",
    "        resized_image = image.resize(desired_size)\n",
    "        resized_image.save(resized_image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EjecuciÃ³n 1\n",
    "DistribuciÃ³n de datos:\n",
    "- 80% -> Datos sin labels para autoencoder\n",
    "- 10% -> Datos con labels (training)\n",
    "- 10% -> Datos con labels (testing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Load data from analysis dataset --\n",
    "\"\"\"\n",
    "\n",
    "parent_folder_path = \"images/resized_dataset/\"\n",
    "\n",
    "# get the categories\n",
    "categories = {}\n",
    "for index, folder_name in enumerate(sorted(os.listdir(parent_folder_path))):\n",
    "    folder_path = os.path.join(parent_folder_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        categories[folder_name] = index\n",
    "\n",
    "# get the amount of images per category\n",
    "category_amount = []\n",
    "for category in categories.keys():\n",
    "    folder_path = os.path.join(parent_folder_path, category)\n",
    "    image_files = os.listdir(folder_path)\n",
    "    category_amount.append(len(image_files))\n",
    "\n",
    "# get images \n",
    "arrays = []\n",
    "for cat_folder, value in categories.items():\n",
    "\n",
    "    folder_path = os.path.join(parent_folder_path, cat_folder)\n",
    "    image_files = os.listdir(folder_path)\n",
    "\n",
    "    for i, file_name in enumerate(image_files):\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        image = Image.open(file_path)\n",
    "        image_array = np.array(image)\n",
    "\n",
    "        # verify all images are of the desired size\n",
    "        if image.size != (64, 64):\n",
    "            print(file_path, \" IS NOT 64x64, it is: \", image.size)\n",
    "            continue\n",
    "\n",
    "        arrays.append(image_array)\n",
    "\n",
    "# generate the labels array\n",
    "arrays_labels = []\n",
    "for i in range(len(categories)):\n",
    "    arrays_labels += [i] * category_amount[i]\n",
    "arrays_labels = np.array(arrays_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Load autoencoder, train and testing data --\n",
    "\n",
    "Using the distribution:\n",
    "80% -> autoencoder\n",
    "10% -> training data\n",
    "10% -> testing data\n",
    "\"\"\"\n",
    "\n",
    "X_auto, X, y_auto, y = train_test_split(arrays, arrays_labels, test_size=0.8, random_state=42, stratify=arrays_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "# normalize \n",
    "X_auto = np.array(X_auto).astype('float32') / 255\n",
    "X_train = np.array(X_train).astype('float32') / 255\n",
    "X_test = np.array(X_test).astype('float32') / 255\n",
    "X = np.array(X).astype('float32') / 255\n",
    "\n",
    "# resize \n",
    "X_auto = X_auto.reshape((len(X_auto), 64, 64, 3))\n",
    "X_train = X_train.reshape((len(X_train), 64, 64, 3))\n",
    "X_test = X_test.reshape((len(X_test), 64, 64, 3))\n",
    "X = X.reshape((len(X), 64, 64, 3))\n",
    "\n",
    "# make labels as lists with possible values\n",
    "y_train = to_categorical(y_train, num_classes=38)\n",
    "y_test = to_categorical(y_test, num_classes=38)\n",
    "y = to_categorical(y, num_classes=38)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificador A -> Sin autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Autoencoder Architecture --\n",
    "\"\"\"\n",
    "\n",
    "input_img = Input(shape=X_auto[0].shape)\n",
    "encoded = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "latent = MaxPooling2D((2, 2), padding='same')(encoded)\n",
    "decoded = Conv2D(16, (3, 3), activation='relu', padding='same')(latent)\n",
    "decoded = UpSampling2D((2,2))(decoded)\n",
    "decoded = Conv2D(3, (3, 3), activation='relu', padding='same')(decoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-13 01:01:26,025]\u001b[0m A new study created in memory with name: no-name-60411624-4020-47e6-a375-5ddef3d92788\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining trial with lr=0.019417802984540277 and epochs=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-13 01:05:11,846]\u001b[0m Trial 0 finished with value: 0.8897082209587097 and parameters: {'lr': 0.019417802984540277, 'epochs': 8}. Best is trial 0 with value: 0.8897082209587097.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining trial with lr=0.002659842896948903 and epochs=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-13 01:10:34,802]\u001b[0m Trial 1 finished with value: 0.8865410685539246 and parameters: {'lr': 0.002659842896948903, 'epochs': 11}. Best is trial 0 with value: 0.8897082209587097.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining trial with lr=0.0031169159482282733 and epochs=14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Autoencoder calculated with CNN --\n",
    "\n",
    "Get the best hyperparameters for the autoencoder using the CNN architecture proposed\n",
    "\"\"\"\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # define hyperparameters to be optimized\n",
    "    lr = trial.suggest_float('lr', 0.001, 0.05, log=True)\n",
    "    epochs = trial.suggest_int('epochs', 5, 25)    \n",
    "\n",
    "    print(f\"Beggining trial with lr={lr} and epochs={epochs}\")\n",
    "\n",
    "    # Compile the autoencoder\n",
    "    optimizer = Adam(learning_rate = lr)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(X_auto, X_auto,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    verbose=0)\n",
    "    \n",
    "    # Evaluate the autoencoder\n",
    "    _, accuracy = autoencoder.evaluate(X_auto, X_auto, verbose=0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create optuna study and optimize the objetive function\n",
    "begin_time = time.time()\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "finish_time = time.time()\n",
    "\n",
    "optuna_time = finish_time - begin_time\n",
    "print(f\"\\nTime taken to find best hyperparams: {optuna_time} s\")\n",
    "# Print the best hyperparameters and the best objective value\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Best Accuracy: \", best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 18s 628ms/step - loss: 207.7034 - accuracy: 0.1958\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 17s 617ms/step - loss: 0.1755 - accuracy: 0.1092\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 17s 622ms/step - loss: 0.1755 - accuracy: 0.1092\n",
      "Epoch 4/10\n",
      " 5/27 [====>.........................] - ETA: 13s - loss: 0.1731 - accuracy: 0.1028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m autoencoder\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMSE\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[39m# Train the autoencoder\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit(X_auto, X_auto,\n\u001b[0;32m     11\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m                 batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m                 shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Juan Navarro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Create autoencoder --\n",
    "\"\"\"\n",
    "\n",
    "# Compile the autoencoder\n",
    "optimizer = Adam(learning_rate = best_params['lr'])\n",
    "autoencoder.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_auto, X_auto,\n",
    "                epochs=best_params['epochs'],\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Test image creation with autoencoder\n",
    "\"\"\"\n",
    "\n",
    "reconstructed_imgs = autoencoder.predict(X_test[:5])\n",
    "\n",
    "n = 5  # nÃºmero de imÃ¡genes a mostrar\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Imagen original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(64, 64,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Imagen reconstruida\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_imgs[i].reshape(64, 64,3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificador B -> Pesos del autoencoder congelados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define encoder\n",
    "\"\"\"\n",
    "encoder = Model(input_img, latent)\n",
    "encoded_train = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del clasificador utilizando el vector latente como entrada\n",
    "input_latent = Input(shape=encoded_train[0].shape)\n",
    "output = Flatten()(input_latent)\n",
    "output = Dense(38, activation='sigmoid')(output)\n",
    "\n",
    "# Crear el modelo del clasificador\n",
    "classifier = Model(input_latent, output)\n",
    "\n",
    "# Compilar el modelo del clasificador\n",
    "optimizer = Adam(learning_rate = 0.0005)\n",
    "classifier.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el clasificador utilizando el vector latente como caracterÃ­sticas de entrada\n",
    "classifier.fit(encoded_train, y_train,\n",
    "               epochs=100,\n",
    "               batch_size=128,\n",
    "               validation_data=(encoded_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluar el rendimiento del clasificador en el conjunto de prueba\n",
    "loss, accuracy = classifier.evaluate(encoded_test, y_test)\n",
    "print(\"PÃ©rdida de clasificaciÃ³n:\", loss)\n",
    "print(\"Exactitud de clasificaciÃ³n:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificador C -> Pesos del autoencoder sin congelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del clasificador utilizando el vector latente como entrada\n",
    "output = Flatten()(latent)\n",
    "output = Dense(38, activation='sigmoid')(output)\n",
    "\n",
    "# Crear el modelo del clasificador\n",
    "classifier = Model(encoder, output)\n",
    "\n",
    "# Compilar el modelo del clasificador\n",
    "optimizer = Adam(learning_rate = 0.0005)\n",
    "classifier.compile(optimizer=optimizer, loss='MSE', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el clasificador utilizando el vector latente como caracterÃ­sticas de entrada\n",
    "classifier.fit(X_train, y_train,\n",
    "               epochs=100,\n",
    "               batch_size=128,\n",
    "               validation_data=(encoded_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluar el rendimiento del clasificador en el conjunto de prueba\n",
    "loss, accuracy = classifier.evaluate(X_test, y_test)\n",
    "print(\"PÃ©rdida de clasificaciÃ³n:\", loss)\n",
    "print(\"Exactitud de clasificaciÃ³n:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
